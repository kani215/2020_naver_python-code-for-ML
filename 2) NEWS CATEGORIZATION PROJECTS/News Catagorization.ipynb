{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# python - 비슷한 뉴스를 선정하는 기법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 문제상황\n",
    "   컴퓨터는 문자를 논리적으로 이해하지는 못함\n",
    "   \n",
    "2. 코드의 논리 \n",
    "   (문자 - 숫자 변경해 이해하는 컴퓨터적 사고인데) 숫자로 유사하다는 이해를 어떻게 컴퓨터에게 지시할까?\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문자를 vector로 one hot encoding =bag of words vector\n",
    "- 하나의 단어를 vector의 index로 인식시키는 방법\n",
    "\n",
    "\n",
    "rome = [1,0,0,0,0,0,0,0,0]\n",
    "paris = [0,1,0,0,0,0,0,0,0]\n",
    "rome, partis = [1,1,0,0,0,0,0,0,0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 유클리드 distance\n",
    "\n",
    "좌표상 거리 차이로 유사성 판단"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cosine distance\n",
    "cosine similarity 로 유사성 판단\n",
    "\n",
    "love, hate 코사인 simiarilty 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data set\n",
    "\n",
    "> 스포츠 기사 데이터 \n",
    "\n",
    "> 1234 야구 5678축구"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process\n",
    "\n",
    "- 파일 불러오기\n",
    "- 파일 읽어서 단어사전 corpus 만들기\n",
    "> 신문 1개 별로 split 하고 , 소문자로 일괄변경, set으로 묶음_단어 종류가 궁금할 것이기 때문 \n",
    "- 단어별로 index만들기\n",
    "- 만들어진 인덱스로 문서별로 bag of words vector 생성\n",
    "> data가 작을 때도 유용함\n",
    "- 비교하고자 하는 문서 비교\n",
    "> 여기서는 cosine similarity 사용 문서 1개와 다른 것들을 비교\n",
    "- 얼마나 맞는지 측정\n",
    "> 정확도 측정도 중요합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 코드 분할 필기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def get_file_list(dir_name):\n",
    "    return os.listdir(dir_name)\n",
    "\n",
    "if __name__ == \"__main__\" :\n",
    "    dir_name=\"news_data\"\n",
    "    file_list = get_file_list(dir_name)\n",
    "    # 폴더 속 파일명들 가져와서 list화\n",
    "    file_list = [os.path.join(dir_name,file_name) for file_name in file_list]\n",
    "    # python에서는 주소를 가져올때 join을 쓰는데 흔히 쓰는 파일 합치기의 기능과 더불어 os에 자동\n",
    "    # 적으로 join을 적용한다는 장점 \n",
    "    #print(file_list) #파일과 폴더명이 합쳐져서 나온다.\n",
    "    \n",
    "    x_text, y_class = get_gontents(file_list)\n",
    "    \n",
    "    corpus = get_corpus_dict(x_text)\n",
    "    # 80개 문서의 모두 단어의 갯수만큼 벡터가 발생합니다.\n",
    "    # 지금 이 분석에서는 데이터 is are am 등 을 처리하지 않았습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파일별로 내용읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contents(file_list):\n",
    "    y_class = []\n",
    "    x_text = []\n",
    "    class_dict = {\n",
    "        1:\"0\",2:'0',3:\"0\",4:\"0\", 5:'1',6:'1',7:'1',8:'1'}\n",
    "    \n",
    "    for file_name in file_list:\n",
    "        try:\n",
    "            f = open(file_name,\"r\",encoding =\"cp949\")\n",
    "            # windows 인코딩 cp949\n",
    "            category = int(file_name.split(os.sep)[1].split(\"_\")[0])\n",
    "            # 아까 위에서 path로 합쳤기 때문에 os.sep기준으로 split하는 것이야\n",
    "            y_class.append(class_dict[category])\n",
    "            x_text.append(f.read())\n",
    "            # x_text 기사 각각의 내용을 다 합쳐두었습니다.\n",
    "            f.close()\n",
    "            except UnicodeDecodeError as e:\n",
    "                print(e)\n",
    "                print(file_name)\n",
    "    return x_text,y_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### corpus 만드기 +단어별 index 생성하기 의미없는 문장부호 제거하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cleaned_text(text): \n",
    "    import re\n",
    "    text = re.sub('\\W+',\"\",text.lower())\n",
    "    return text\n",
    "    # 소문자 정렬 후. re.sub는 측수부호 제거\n",
    "    # 이 작업은 꼭 거치면 좋은게 love와 love. 이 같은 단어라는 인식을 시켜야하기 때문이다.\n",
    "    # 좀더 심화일때는 is are am 을 동일하게 여기는 전처리 기술도 익혀야 겟다고 생각\n",
    "    \n",
    "def get_corpus_dict(text):\n",
    "    text = [sentence.split() for sentence in text]\n",
    "    clenad_words = [get_cleaned_text(word) for words in text for word in words]\n",
    "    \n",
    "    # text(파일 제목을 받으면) split해주고 text에 다시 list로 넣어줌\n",
    "    # 뒤의 이중 for 가 빡신데 text에서 단어들을 뽑고 word다시 한글자씩 뽑아준다.\n",
    "    # 그걸 모아둔게 clenad_words\n",
    "    \n",
    "    \n",
    "    from collections import OrderedDict\n",
    "    corpus_dict = OrderedDict()\n",
    "    for i,v in enumerate(set(clenad_words)):\n",
    "        corpus_dict[v] = i\n",
    "        return corpus_dict\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문서별로 bag of words vector 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count_vector(text,corpus):\n",
    "    text = [sentence.split() for sentence in text]\n",
    "    word_number_list = [[corpus[get_cleaned_text(word)] for word in words]\n",
    "                        for words in text]\n",
    "    x_vector =[[0 for _ in range(len(corpus))] for x in range(len(text))]\n",
    "    # 무식하게 0으로된 리스트 생성 3500이면 0이 3500개\n",
    "    \n",
    "    for i,text in enumerate(word_number_list):\n",
    "        for word_number in text:\n",
    "            x_vector[i][word_number] +=1\n",
    "    return x_vector\n",
    "    # 위에 설정해둔 큰 vector에 word 나온 만큼 해당 index에 1씩 더해준다.\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 비교하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def get_cosine_similariy(x_vector,source):\n",
    "    source_vector =x_vector[source]\n",
    "    similarity_list =[]\n",
    "    for target_vector in x_vector:\n",
    "        similarity_list.append(get_cosine_similarity(source_vector, target_vector))\n",
    "        \n",
    "    return similarity_list\n",
    "\n",
    "def get_top_n_similarity_news(similarity_score,n):\n",
    "    import operator\n",
    "    x ={i:v for i,v in enumerate(similarity_score)}\n",
    "    sorted_x = sorted(x.items(),key=operator.itemgetter(1))\n",
    "    \n",
    "    return list(reversed(sorted_x))[1,n+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def get_file_list(dir_name):\n",
    "    return os.listdir(dir_name)\n",
    "\n",
    "def get_conetents(file_list):\n",
    "    y_class = []\n",
    "    X_text = []\n",
    "    class_dict = {\n",
    "        1: \"0\", 2: \"0\", 3:\"0\", 4:\"0\", 5:\"1\", 6:\"1\", 7:\"1\", 8:\"1\"}\n",
    "\n",
    "    for file_name in file_list:\n",
    "        try:\n",
    "            f = open(file_name, \"r\",  encoding=\"cp949\")\n",
    "            category = int(file_name.split(os.sep)[1].split(\"_\")[0])\n",
    "            y_class.append(class_dict[category])\n",
    "            X_text.append(f.read())\n",
    "            f.close()\n",
    "        except UnicodeDecodeError as e:\n",
    "            print(e)\n",
    "            print(file_name)\n",
    "    return X_text, y_class\n",
    "\n",
    "\n",
    "\n",
    "def get_cleaned_text(text):\n",
    "    import re\n",
    "    text = re.sub('\\W+','', text.lower() )\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_corpus_dict(text):\n",
    "    text = [sentence.split() for sentence in text]\n",
    "    clenad_words = [get_cleaned_text(word) for words in text for word in words]\n",
    "\n",
    "    from collections import OrderedDict\n",
    "    corpus_dict = OrderedDict()\n",
    "    for i, v in enumerate(set(clenad_words)):\n",
    "        corpus_dict[v] = i\n",
    "    return corpus_dict\n",
    "\n",
    "\n",
    "def get_count_vector(text, corpus):\n",
    "    text = [sentence.split() for sentence in text]\n",
    "    word_number_list = [[corpus[get_cleaned_text(word)] for word in words] for words in text]\n",
    "    X_vector = [[0 for _ in range(len(corpus))] for x in range(len(text))]\n",
    "\n",
    "    for i, text in enumerate(word_number_list):\n",
    "        for word_number in text:\n",
    "            X_vector[i][word_number] += 1\n",
    "    return X_vector\n",
    "\n",
    "import math\n",
    "def get_cosine_similarity(v1,v2):\n",
    "    \"compute cosine similarity of v1 to v2: (v1 dot v2)/{||v1||*||v2||)\"\n",
    "    sumxx, sumxy, sumyy = 0, 0, 0\n",
    "    for i in range(len(v1)):\n",
    "        x = v1[i]; y = v2[i]\n",
    "        sumxx += x*x\n",
    "        sumyy += y*y\n",
    "        sumxy += x*y\n",
    "    return sumxy/math.sqrt(sumxx*sumyy)\n",
    "\n",
    "def get_similarity_score(X_vector, source):\n",
    "    source_vector = X_vector[source]\n",
    "    similarity_list = []\n",
    "    for target_vector in X_vector:\n",
    "        similarity_list.append(\n",
    "            get_cosine_similarity(source_vector, target_vector))\n",
    "    return similarity_list\n",
    "\n",
    "\n",
    "def get_top_n_similarity_news(similarity_score, n):\n",
    "    import operator\n",
    "    x = {i:v for i, v in enumerate(similarity_score)}\n",
    "    sorted_x = sorted(x.items(), key=operator.itemgetter(1))\n",
    "\n",
    "    return list(reversed(sorted_x))[1:n+1]\n",
    "\n",
    "def get_accuracy(similarity_list, y_class, source_news):\n",
    "    source_class = y_class[source_news]\n",
    "\n",
    "    return sum([source_class == y_class[i[0]] for i in similarity_list]) / len(similarity_list)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dir_name = \"news_data\"\n",
    "    file_list = get_file_list(dir_name)\n",
    "    file_list = [os.path.join(dir_name, file_name) for file_name in file_list]\n",
    "\n",
    "    X_text, y_class = get_conetents(file_list)\n",
    "\n",
    "    corpus = get_corpus_dict(X_text)\n",
    "    print(\"Number of words : {0}\".format(len(corpus)))\n",
    "    X_vector = get_count_vector(X_text, corpus)\n",
    "    source_number = 10\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for i in range(80):\n",
    "        source_number = i\n",
    "\n",
    "        similarity_score = get_similarity_score(X_vector, source_number)\n",
    "        similarity_news = get_top_n_similarity_news(similarity_score, 10)\n",
    "        accuracy_score = get_accuracy(similarity_news, y_class, source_number)\n",
    "        result.append(accuracy_score)\n",
    "    print(sum(result) / 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 통합 완성 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words : 4024\n",
      "0.6950000000000001\n"
     ]
    }
   ],
   "source": [
    "# 1. 데이터 읽어오기 -----------------------------------------------\n",
    "import os\n",
    "\n",
    "\n",
    "def get_file_list(dir_name):\n",
    "    return os.listdir(dir_name)\n",
    "\n",
    "# 2. 파일별로 내용 읽어오기 -----------------------------------------\n",
    "def get_conetents(file_list):\n",
    "    y_class = []\n",
    "    X_text = []\n",
    "    class_dict = {\n",
    "        1: \"0\", 2: \"0\", 3:\"0\", 4:\"0\", 5:\"1\", 6:\"1\", 7:\"1\", 8:\"1\"}\n",
    "\n",
    "    for file_name in file_list:\n",
    "        try:\n",
    "            f = open(file_name, \"r\",  encoding=\"cp949\")\n",
    "            # windows 인코딩 cp949, open(파일명, 읽기상태, 인코딩)\n",
    "            category = int(file_name.split(os.sep)[1].split(\"_\")[0])\n",
    "            # 아까 위에서 os path로 합쳤기 때문에 os.sep 기준으로 split하는 것임\n",
    "            y_class.append(class_dict[category])\n",
    "            X_text.append(f.read())\n",
    "            # x_text에는 기사의 내용이 다 합쳐져 있습니다.\n",
    "            f.close()\n",
    "        except UnicodeDecodeError as e:\n",
    "            print(e)\n",
    "            print(file_name)\n",
    "    return X_text, y_class\n",
    "\n",
    "\n",
    "# 3. 전처리 강화\n",
    "def get_cleaned_text(text):\n",
    "    import re\n",
    "    text = re.sub('\\W+','', text.lower() )\n",
    "    return text\n",
    "    # 소문자 정렬 후 re.sub는 촉수부호 제거\n",
    "    # 이 작업은 꼭 거치면 좋은게 love와 love. 같은 단어를 다른 단어로 인식 시키는 것을 막기 위함\n",
    "    # 좀 더 심화는 is are am같은 것을 제거해주어야함.\n",
    "\n",
    "# 4. corpus 만들기 + 단어별 index 생성하기 \n",
    "def get_corpus_dict(text):\n",
    "    text = [sentence.split() for sentence in text]\n",
    "    clenad_words = [get_cleaned_text(word) for words in text for word in words]\n",
    "    \n",
    "    # text(파일 제목을 받으면) split 해주고 text에 다시 list로 넣어줌\n",
    "    # 뒤의 이중 for 가 빡센데 text에서 단어들을 뽑고: word, 다시 한글자씩 뽑아준다.\n",
    "    # 그걸 모아둔게 clenad_words\n",
    "    \n",
    "    from collections import OrderedDict\n",
    "    corpus_dict = OrderedDict()\n",
    "    for i, v in enumerate(set(clenad_words)):\n",
    "        corpus_dict[v] = i\n",
    "    return corpus_dict\n",
    "\n",
    "\n",
    "def get_count_vector(text, corpus):\n",
    "    text = [sentence.split() for sentence in text]\n",
    "    word_number_list = [[corpus[get_cleaned_text(word)] for word in words] for words in text]\n",
    "    X_vector = [[0 for _ in range(len(corpus))] for x in range(len(text))]\n",
    "    # 무식하게 0으로된 리스트 생성 3500이면 0이 3500개 (아주 불편한 수법이지만 )\n",
    "    for i, text in enumerate(word_number_list):\n",
    "        for word_number in text:\n",
    "            X_vector[i][word_number] += 1\n",
    "            \n",
    "    # enumerate 수법으로 한번에 그 위치에 값을 증가시켰다.\n",
    "    return X_vector\n",
    "\n",
    "import math\n",
    "def get_cosine_similarity(v1,v2):\n",
    "    \"compute cosine similarity of v1 to v2: (v1 dot v2)/{||v1||*||v2||)\"\n",
    "    sumxx, sumxy, sumyy = 0, 0, 0\n",
    "    for i in range(len(v1)):\n",
    "        x = v1[i]; y = v2[i]\n",
    "        sumxx += x*x\n",
    "        sumyy += y*y\n",
    "        sumxy += x*y\n",
    "    return sumxy/math.sqrt(sumxx*sumyy)\n",
    "\n",
    "def get_similarity_score(X_vector, source):\n",
    "    source_vector = X_vector[source]\n",
    "    similarity_list = []\n",
    "    for target_vector in X_vector:\n",
    "        similarity_list.append(\n",
    "            get_cosine_similarity(source_vector, target_vector))\n",
    "    return similarity_list\n",
    "\n",
    "\n",
    "def get_top_n_similarity_news(similarity_score, n):\n",
    "    import operator\n",
    "    x = {i:v for i, v in enumerate(similarity_score)}\n",
    "    sorted_x = sorted(x.items(), key=operator.itemgetter(1))\n",
    "\n",
    "    return list(reversed(sorted_x))[1:n+1]\n",
    "\n",
    "def get_accuracy(similarity_list, y_class, source_news):\n",
    "    source_class = y_class[source_news]\n",
    "\n",
    "    return sum([source_class == y_class[i[0]] for i in similarity_list]) / len(similarity_list)\n",
    "\n",
    "\n",
    "# -------------- 실행되는 메인 파일 -----------------\n",
    "if __name__ == \"__main__\":\n",
    "    dir_name = \"news_data\"\n",
    "    file_list = get_file_list(dir_name)\n",
    "    # 폴더 속 파일명들을 가져와서 list화 시킨다. \n",
    "    file_list = [os.path.join(dir_name, file_name) for file_name in file_list]\n",
    "    # python에서는 주소를 가져올 때 join을 쓰는데 흔히 쓰는 파일 합치기의 기능과 더불어 os종류에 관계없이\n",
    "    # 자동적으로 join을 적용\n",
    "    X_text, y_class = get_conetents(file_list)\n",
    "\n",
    "    corpus = get_corpus_dict(X_text)\n",
    "    print(\"Number of words : {0}\".format(len(corpus)))\n",
    "    X_vector = get_count_vector(X_text, corpus)\n",
    "    source_number = 10\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for i in range(80):\n",
    "        source_number = i\n",
    "\n",
    "        similarity_score = get_similarity_score(X_vector, source_number)\n",
    "        similarity_news = get_top_n_similarity_news(similarity_score, 10)\n",
    "        accuracy_score = get_accuracy(similarity_news, y_class, source_number)\n",
    "        result.append(accuracy_score)\n",
    "    print(sum(result) / 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scikit- learn을 이용해서 단어를 뽑아서 벡터화 하여 유사성 비교\n",
    "> 위의 방법은 너무 구리며 실전에서는 이렇게 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 0 1 0]\n",
      " [0 1 1 0 1 1]\n",
      " [0 1 1 1 1 0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['black', 'cow', 'is', 'red', 'this', 'white']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "corpus = [\"this is black cow\", \"this is white cow\",\"this is red cow\"]\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(X.toarray())\n",
    "\n",
    "vectorizer.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    dir_name = \"news_data\"\n",
    "    file_list = get_file_list(dir_name)\n",
    "    # 폴더 속 파일명들을 가져와서 list화 시킨다. \n",
    "    file_list = [os.path.join(dir_name, file_name) for file_name in file_list]\n",
    "    # python에서는 주소를 가져올 때 join을 쓰는데 흔히 쓰는 파일 합치기의 기능과 더불어 os종류에 관계없이\n",
    "    # 자동적으로 join을 적용\n",
    "    X_text, y_class = get_conetents(file_list)\n",
    "\n",
    "    corpus = get_corpus_dict(X_text)\n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
